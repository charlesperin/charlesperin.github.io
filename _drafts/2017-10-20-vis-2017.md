---
date: '2017-10-20 00:00:00 -0500'
layout: blog_post
categories: blog

title: VIS Insider 2017
subtitle: ""
thumb: /images/blog/2017_vis-thumb.jpg
teaser: /images/blog/2017_vis-teaser.jpg
description_short: Every year in the fall happens the largest academic conference on visualization. I attended VIS for the 6th time. And for the first time, I decided to write down my two cents on the conference.

---


### Test of Time Awards

Viegas and Wattenberg

Stasko Jigsaw, review process, directions of the conference. Thought provoking. Personal thoughts.


### Interaction

Was nice to see some interaction papers.


### VDS vis
is it the same thing than what HCI people did when they stood against AI in the 80s-90s? How come some people are still discussing whether or not we "should open the black box"? How come some people truly think we should have machines take decisions?
Buz thing


### Evaluation

* 1 session called evaluation, but almost all VIS papers had one (see Haroz blog post about that). That's good, and that's terrible. Good because we, as a communauty, get better and better at running experiments, interpreting and communicating results. That's terrible because we are making it a requirement, we are putting blinders, and disregard valuable work just because it is not a quantitative experiment. There was this panel where vision science meets visualization, which seemed really nice (could not attend). I truly think we have much to learn from vision science. I also truly think that visualization is not just about vision science. We have to be careful to keep an open and curious mind for exciting work. There was a panel about diversity in VIS (that I also missed), and we should also remember that our research field is diverse, and promote the diversity of research papers. 
* I do not have anything against controlled experiments (I myself had a VIS paper this year which is basically a study paper). But controlled experiments cannot address all research questions (high internal validity but low external validity, high precision but low realism, good at capturing narrow, well defined measures but bad at capturing the whole picture), there are other ways of doing research, and also of validating results. Working with (even very few) experts, qualitative studies (spoiler: qualitative studies are not just some pretty preliminary studies) - see the qualitative study tutorials at VIS, observations, etc..
We are moving away from p-values (thanks Pierre), we see many analyses based on confidence intervals, and a few Bayesian approaches [kays]. That's good. But this is just one specific way of gathering and analyzing empirical data. Gathering empirical is not equivalent to running a controlled study, the latter is just a particular case of the former.
* Look at all papers and give numbers regarding numbers of evaluations. Would love to compare that number with % of submitted papers with evaluation. What is the acceptance rate at VIS for papers with low-level, quantitative evaluation / without evaluation? Remind panel vision science meets visualization, different from visualization should become vision science. Let's collaborate.

### Output

Many discussions.

Many cool project ideas.




